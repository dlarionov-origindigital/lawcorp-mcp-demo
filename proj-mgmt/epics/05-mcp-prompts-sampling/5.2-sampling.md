# 5.2: Sampling (Server-Initiated LLM Calls)

**Status:** BACKLOG
**Epic:** [Epic 5: MCP Prompts & Sampling](./_epic.md)
**Goal:** Implement server-initiated LLM sampling requests that enhance tool capabilities with AI reasoning â€” document classification, deadline extraction, conflict detection, and time entry enrichment.

## Items

| ID | Title | Type | Status |
|---|---|---|---|
| [5.2.1](./5.2.1-document-classification-sampling.md) | Document classification sampling | Story | BACKLOG |
| [5.2.2](./5.2.2-deadline-extraction-sampling.md) | Deadline extraction sampling | Story | BACKLOG |
| [5.2.3](./5.2.3-conflict-detection-sampling.md) | Conflict detection enhancement sampling | Story | BACKLOG |
| [5.2.4](./5.2.4-time-entry-enhancement-sampling.md) | Time entry description enhancement sampling | Story | BACKLOG |

## Acceptance Criteria

- [ ] Each sampling use case calls `sampling/createMessage` via the MCP SDK
- [ ] Sampling results are used to enrich stored data, not just returned to the client
- [ ] Sampling failures degrade gracefully (tool/workflow still completes)
